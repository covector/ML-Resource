<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="shortcut icon" href="../images/logo.ico">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;400;500;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles/main.css">
    <link rel="stylesheet" href="../styles/topBar.css">
    <link rel="stylesheet" href="../styles/welcome.css">
    <link rel="stylesheet" href="../styles/paper.css">
    <script src="../script/scroll.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Staatliches&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;400;500;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Quicksand:wght@300&display=swap" rel="stylesheet">
    <title>Policy Gradient</title>
</head>
<body>
    <div id="topBar"><div>Policy Gradient</div></div>
    <div id="welcomeBox" class="policyGradient"><div id="logo">Policy Gradient</div><div class="primary" id="welcomePrimary">Policy Gradient</div><div class="secondary" id="welcomeSecondary">Policy Gradient is an algorithm that optimize the policy directly using the estimated gradient.</div></div>
    <a href="../policy gradient/trust region policy optimization.pdf" target="_blank"><div class="paper odd"><div class="title">Trust Region Policy Optimization</div><div class="author">Schulman et al.</div></div></a>
    <a href="../policy gradient/proximal policy optimization.pdf" target="_blank"><div class="paper even"><div class="title">Proximal Policy Optimization Algorithms</div><div class="author">Schulman et al.</div></div></a>
    <a href="../policy gradient/policy gradient theorem.pdf" target="_blank"><div class="paper odd"><div class="title">Policy Gradient Beamer</div><div class="author">Ashwin Rao</div></div></a>
    <a href="../policy gradient/policy gradient theorem paper.pdf" target="_blank"><div class="paper even"><div class="title">Policy Gradient Methods for Reinforcement Learning with Function Approximation</div><div class="author">Sutton et al.</div></div></a>
    <a href="../policy gradient/deterministic pg.pdf" target="_blank"><div class="paper odd"><div class="title">Deterministic Policy Gradient Algorithms</div><div class="author">Silver et al.</div></div></a>
    <a href="../policy gradient/deterministic pg proof.pdf" target="_blank"><div class="paper even"><div class="title">Deterministic Policy Gradient Algorithms: Supplementary Material</div><div class="author">Silver et al.</div></div></a>
    <a href="../policy gradient/generalised advantage estimation.pdf" target="_blank"><div class="paper odd"><div class="title">High-Demensional Continuous Control Using Generalized Advantage Estimation</div><div class="author">Schulman et al.</div></div></a>
    <a href="../policy gradient/comparing policy gradient.pdf" target="_blank"><div class="paper even"><div class="title">Comparing Policy-Gradient Algorithms</div><div class="author">Sutton et al.</div></div></a>
</body>
</html>